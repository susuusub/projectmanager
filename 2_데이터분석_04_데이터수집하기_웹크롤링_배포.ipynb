{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/susuusub/projectmanager/blob/main/2_%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D_04_%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%88%98%EC%A7%91%ED%95%98%EA%B8%B0_%EC%9B%B9%ED%81%AC%EB%A1%A4%EB%A7%81_%EB%B0%B0%ED%8F%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2d8f562",
      "metadata": {
        "id": "d2d8f562"
      },
      "source": [
        "# 데이터 분석"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d90d2c2a",
      "metadata": {
        "id": "d90d2c2a"
      },
      "source": [
        "## 4. 웹 크롤링으로 데이터 수집"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29efc198",
      "metadata": {
        "id": "29efc198"
      },
      "source": [
        "---------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22449e9b",
      "metadata": {
        "id": "22449e9b"
      },
      "source": [
        "### 01.웹 크롤링의 이해\n",
        "- **웹 크롤링(Web Crawling)**: 웹 페이지로부터 원하는 정보를 추출하는 기법, 보통 여러 페이지 탐색하는 방법,\n",
        "- **웹 스크래핑(Web Scraping)**: 특정한 하나의 웹 페이지를 탐색하는 방법\n",
        "- **웹 크롤링과 웹 스크래핑의  방법이 동일하여 보통 웹 크롤링이라고 통칭해서 부른다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45d36f92",
      "metadata": {
        "id": "45d36f92"
      },
      "source": [
        "### 라이브러리 설치하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d254f56b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d254f56b",
        "outputId": "1f9aeb5c-bd6c-40af-877b-f18dbbc10c1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "# 정적 크롤링을 위한 requests 설치\n",
        "# url로부터 html 소스코드 요청\n",
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "83fff2f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83fff2f7",
        "outputId": "311a558e-6eac-4189-fdea-776f1c8bf6cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.4.1)\n"
          ]
        }
      ],
      "source": [
        "# HTML과 XML 문서를 파싱하기 위한 파이썬 패키지\n",
        "!pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bb5d63be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb5d63be",
        "outputId": "105c2f55-dedc-464f-bbd7-e74a4e69fa50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.11.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.4)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.22.2-py3-none-any.whl (400 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.2/400.2 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.10.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=20.1.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.4)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.1.3)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.2.0 selenium-4.11.2 trio-0.22.2 trio-websocket-0.10.3 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "# 동적 크롤링을 위한 셀레니움 설치\n",
        "!pip install selenium"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e298def",
      "metadata": {
        "id": "8e298def"
      },
      "source": [
        "### 라이브러리 확인하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8d82eb73",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d82eb73",
        "outputId": "72ab1e4c-0d87-4fcd-baab-4e0c99e48c5c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "requests.get(\"https://www.naver.com\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d77bdc03",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d77bdc03",
        "outputId": "26d15683-313c-42a6-b3a6-0e82bf8b3f2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#응답 상태:  200\n",
            "#응답 헤더유형:  text/html; charset=ISO-8859-1\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "response = requests.get(\"https://google.com\")\n",
        "\n",
        "# 응답 상태\n",
        "print('#응답 상태: ', response.status_code)\n",
        "\n",
        "# 응답 바이너리 원문\n",
        "#print('#응답 바이너리 원문: ', response.content) #결과값 - html코드\n",
        "\n",
        "# 응답 UTF-8로 인코딩된 문자열\n",
        "#print('#응답 UTF-8로 인코딩된 문자열: ', response.text)\n",
        "\n",
        "# 응답 헤더\n",
        "#print('#응답 헤더: ', response.headers)\n",
        "\n",
        "# # 응답 헤더: 콘텐트 유형\n",
        "#print('#응답 헤더유형: ', response.headers['Content-Type'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "414c226e",
      "metadata": {
        "id": "414c226e"
      },
      "source": [
        "-------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5629cbbb",
      "metadata": {
        "id": "5629cbbb"
      },
      "source": [
        "## 02.정적 크롤링(스크래핑)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a29d08f",
      "metadata": {
        "id": "7a29d08f"
      },
      "source": [
        "### [실습] 네이버 뉴스 검색 데이터 수집하기\n",
        "단, 네이버에서는 한번에 90개까지만 수집 가능한 상태임 <br>\n",
        "- **1.데이터 가져올 웹페이지**: 네이버 뉴스검색(검색어:우영우) https://search.naver.com/search.naver?where=news&sm=tab_jum&query=%EC%9A%B0%EC%98%81%EC%9A%B0\n",
        "- **2.가져올 정보**: 기사제목, URL\n",
        "- **3.DataFrame으로 만들기**\n",
        "- **4.DataFrame --> csv파일로 만들기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "beaa6805",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "beaa6805",
        "outputId": "6bcbc80a-81f4-4c26-c683-249fde1c3b8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "#네이버 뉴스 데이터 크롤링하기...\n",
            "--------------------------------------------------\n",
            "검색 키워드를 입력하세요 : 손흥민\n",
            "<Response [403]>\n",
            "\n",
            "검색어[손흥민] 로 검색된 뉴스 [0]건 가져오기 완료!\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [title, url]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2089f228-05f7-4970-b9c8-3e8bd3c18690\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2089f228-05f7-4970-b9c8-3e8bd3c18690')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2089f228-05f7-4970-b9c8-3e8bd3c18690 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2089f228-05f7-4970-b9c8-3e8bd3c18690');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "print('-'*50)\n",
        "print(\"#네이버 뉴스 데이터 크롤링하기...\")\n",
        "print('-'*50)\n",
        "\n",
        "# 검색 URL 지정하기\n",
        "# query=검색어, start=페이지 (10건씩 추출)\n",
        "news_url = 'https://search.naver.com/search.naver?where=news&query={}&start='\n",
        "\n",
        "# 검색어 직접 입력 받기\n",
        "keyword = input('검색 키워드를 입력하세요 : ')\n",
        "query = keyword.replace(' ', '+') # 검색에서 공백문자를 +로 대체해 사용하도록 적용\n",
        "\n",
        "\n",
        "def get_naverNewTitle(total=100, start_cnt=1):\n",
        "\n",
        "    # 1.웹 데이터 수집하기\n",
        "    titles, urls= [],[]     # 제목=[], url=[] 데이터담기\n",
        "    while True:\n",
        "\n",
        "        # 웹 페이지 요청하기\n",
        "\n",
        "        response = requests.get(news_url.format(query) + str(start_cnt))\n",
        "        print (response)\n",
        "\n",
        "        # 2.응답 결과 파싱하기(제목, url 추출하기)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "#         page_tags   = soup.select('ul.list_news > li > div > div > a')\n",
        "        page_tags   = soup.select('div.news_area > a')\n",
        "        page_titles = list(map(lambda tag: tag.get('title'), page_tags))\n",
        "        page_urls   = list(map(lambda tag: tag.get('href'), page_tags))\n",
        "\n",
        "\n",
        "        # 목적 개수만큼 추출하기\n",
        "        if len(page_titles) <= 0:\n",
        "            break\n",
        "        elif start_cnt >= total:\n",
        "            break\n",
        "        else:\n",
        "            titles += page_titles\n",
        "            urls += page_urls\n",
        "            start_cnt += len(page_titles)\n",
        "\n",
        "        # 데이터 수집 진행률 표시\n",
        "        print(f\"{int( start_cnt-1/total*100 ) }% \", end=\"\")\n",
        "\n",
        "    print()\n",
        "    print(f\"검색어[{keyword}] 로 검색된 뉴스 [{len(titles)}]건 가져오기 완료!\")\n",
        "    print('-'*50)\n",
        "\n",
        "    # 3.titles, urls 2차원 리스트로 만들어 DataFrame으로 만든다.\n",
        "    datas = [[t, u] for t, u in zip(titles, urls)]\n",
        "    df = pd.DataFrame(datas, columns=['title','url'])\n",
        "    return df\n",
        "\n",
        "df = get_naverNewTitle(10, 1)\n",
        "df\n",
        "\n",
        "# 4.파일로 저장하기\n",
        "file = f\"./sample_data/naver_news_{keyword}_title.csv\"\n",
        "news_df = df['title']\n",
        "news_df.to_csv(file, index=False, encoding=\"utf-8-sig\")\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01a2f4c0",
      "metadata": {
        "id": "01a2f4c0"
      },
      "source": [
        "### [실습] (수집된 데이터)기사 제목의 단어(토큰) 빈도수 분석하기\n",
        "- ** CountVectorizer 클래스 사용 : 단어 빈도수 추출**\n",
        "- **단, 한글에서 불용어 처리 및 가중치 처리 등 자연어 텍스트 전처리에 필요한 여러 가지 방법은 여기서 다루지 않는다.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b1d8c130",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1d8c130",
        "outputId": "f24caee8-5cfc-4204-800c-e04b754766d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f0d82316",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "f0d82316",
        "outputId": "9a54169c-7b65-422d-9a70-4e3e2e7e6f59"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.2.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import sklearn\n",
        "sklearn.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1cca190",
      "metadata": {
        "scrolled": true,
        "id": "b1cca190"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
        "\n",
        "\n",
        "def get_wordTokenCount(corpus):\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "    # 기사제목을 토큰화\n",
        "    vect = CountVectorizer().fit(corpus)\n",
        "    count = vect.transform(corpus).toarray().sum(axis=0)\n",
        "\n",
        "    # 토큰 빈도수로 정렬하고 토큰명 추출\n",
        "    idx = np.argsort(-count)  # 내림 정렬하여 인덱스 반환: 토큰의 인덱스\n",
        "    count = count[idx]        # 토큰의 빈도수\n",
        "    feature_name = np.array(vect.get_feature_names_out())[idx]  # 토큰값\n",
        "\n",
        "    # 빈도수 많은 순서대로 토큰명 10개만 출력\n",
        "    print(list(zip(feature_name, count))[:10])\n",
        "\n",
        "    return feature_name, count\n",
        "\n",
        "\n",
        "def draw_wordTokenCountGraph(data, freq):\n",
        "    plt.bar(data, freq)\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    # 그래프 그림 저장히기\n",
        "    plt.savefig(f'image/{keyword}_bar_graph.png')\n",
        "\n",
        "\n",
        "# 기사제목을 말뭉치로 사용\n",
        "corpus = df['title'].to_list()\n",
        "# print(corpus)\n",
        "\n",
        "# 기사제목을 토큰화하여 빈도수 가져오기\n",
        "feature_name, count = get_wordTokenCount(corpus)\n",
        "\n",
        "# 단어(토큰) 빈도 수_Bar그래프 그리기 : 상위 10개\n",
        "draw_wordTokenCountGraph(feature_name[:10], count[:10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9efc23f7",
      "metadata": {
        "id": "9efc23f7"
      },
      "source": [
        "### [실습] (수집된 데이터)기사 제목의 토큰 빈도수 워드클라우드로 시각화\n",
        "- 워드클라우드 입력데이터 : 딕셔너리 타입"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bd2bb37",
      "metadata": {
        "scrolled": true,
        "id": "0bd2bb37"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = (10,6)\n",
        "\n",
        "# 자신의 컴퓨터 환경에 맞는 한글 폰트 경로를 설정(window에서)\n",
        "# font_path = 'malgun'  # C:/Windows/Fonts/\n",
        "font_path = 'HMKMMAG' # C:/Windows/Fonts/HMKMMAG.TTF\n",
        "\n",
        "# (토큰명, 빈도수) 딕셔너리 타입으로 변환\n",
        "data = dict(zip(feature_name, count))\n",
        "\n",
        "# 워드클라우드로 그래프로 시각화\n",
        "wc = WordCloud(width = 1000, height = 600, background_color=\"white\", font_path=font_path)\n",
        "plt.imshow(wc.generate_from_frequencies(data)) #딕셔너리\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# 파일로 저장하기\n",
        "wc.to_file(f'image/{keyword}_워드클라우드.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ed157a1",
      "metadata": {
        "id": "4ed157a1"
      },
      "source": [
        "### [실습] (수집된 데이터)기사 제목의 토큰 빈도수 출력하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06bd2351",
      "metadata": {
        "scrolled": true,
        "id": "06bd2351"
      },
      "outputs": [],
      "source": [
        "data = dict(zip(feature_name, count))\n",
        "token_df = pd.DataFrame(data, index=[1])\n",
        "token_df\n",
        "\n",
        "# token_df.to_csv('data/token.csv', index=False, encoding=\"utf-8-sig\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66062103",
      "metadata": {
        "id": "66062103"
      },
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9056f1dd",
      "metadata": {
        "id": "9056f1dd"
      },
      "source": [
        "## 03.동적 크롤링(PC에서 실행)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89787e7c",
      "metadata": {
        "id": "89787e7c"
      },
      "source": [
        "- https://chromedriver.chromium.org/\n",
        "- 최신 chrome webdriver 다운로드\n",
        "- 해당 위치에 WebDriver폴더 만들고 exe파일 옮겨놓는다.\n",
        "-  (C:/python/projectmanager/WebDriver/chromedriver.exe)\n",
        "- 자신의 크롬 웹 브라우저의 버전을 확인하고 버전에 맞는 것을 다운로드해야한다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebe44ce3",
      "metadata": {
        "id": "ebe44ce3"
      },
      "source": [
        "#### # webdriver 동작 테스트하기\n",
        "- 자신의 크롬 웹 브라우저의 버전을 확인하고 버전에 맞는 것을 다운로드해야한다. 그렇지 않으면 오류가 발생한다.\n",
        "- 아래 코드를 실행시키면 크롬 브라우져가 잠깐 실행되었다 닫힌다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ec97e54",
      "metadata": {
        "id": "4ec97e54"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "\n",
        "# chromedriver.exe 파일이 있는 경로\n",
        "driver = './WebDriver/chromedriver.exe'\n",
        "wd = webdriver.Chrome(driver)\n",
        "\n",
        "wd.get('https://www.naver.com/')\n",
        "\n",
        "wd.close()  # 브라우저가 실행되었다가 자동으로 닫힌다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e47078f",
      "metadata": {
        "id": "8e47078f"
      },
      "source": [
        "### [실습]  커피빈매장 정보 크롤링하여 파일로 저장하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fabdfa25",
      "metadata": {
        "id": "fabdfa25"
      },
      "source": [
        "- 아래 사이트를 이용해 호출해야할 자바스크립트 함수를 확인하다.\n",
        "- https://www.coffeebeankorea.com\n",
        "- https://www.coffeebeankorea.com/store/store.asp\n",
        "- (매장 번호로) 자세히보기: javascript:storePop2('374');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3426db71",
      "metadata": {
        "scrolled": true,
        "id": "3426db71"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "from selenium import webdriver\n",
        "import time\n",
        "\n",
        "MAX = 10     # 추출 데이터 건수\n",
        "\n",
        "#[매장 추출 함수]\n",
        "def getStoreInfo():\n",
        "    CoffeeBean_URL = \"https://www.coffeebeankorea.com/store/store.asp\"\n",
        "    wd = webdriver.Chrome('./WebDriver/chromedriver.exe')\n",
        "\n",
        "    result = []  # 데이터 저장 변수\n",
        "    total, cnt = 370, 0\n",
        "    for i in range(1, total+1):  #매장 수 만큼(370) 반복\n",
        "        wd.get(CoffeeBean_URL)\n",
        "        time.sleep(1)  #웹페이지 연결할 동안 1초 대기\n",
        "        try:\n",
        "            wd.execute_script(\"storePop2(%d)\" %i)\n",
        "            time.sleep(1) #스크립트 실행 할 동안 1초 대기\n",
        "\n",
        "            html = wd.page_source\n",
        "            soupCB = BeautifulSoup(html, 'html.parser')\n",
        "            store_name_h2 = soupCB.select(\"div.store_txt > h2\")\n",
        "            store_name = store_name_h2[0].string  #매장 이름\n",
        "\n",
        "            store_info = soupCB.select(\"div.store_txt > table.store_table > tbody > tr > td\")\n",
        "            store_address_list = list(store_info[2])\n",
        "            store_address = store_address_list[0]  #매장 주소\n",
        "\n",
        "            store_phone = store_info[3].string     #매장 전화번호\n",
        "            result.append([store_name]+[store_address]+[store_phone])\n",
        "            cnt += 1\n",
        "            # 매장정보 가져온 데이터 출력하기\n",
        "            print(\"[%3d] %3d - %s\" % (cnt, i, store_name))\n",
        "\n",
        "\n",
        "             # MAX값에 해당하는 건수 만큼만 실행하기\n",
        "            if cnt >= MAX:\n",
        "                break\n",
        "\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    return result\n",
        "\n",
        "#---------------\n",
        "# main\n",
        "#---------------\n",
        "print('CoffeeBean store crawling >>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
        "result = getStoreInfo()  #[매장 추출 함수]호출하기\n",
        "\n",
        "df_cb = pd.DataFrame(result, columns=('store', 'address','phone'))\n",
        "df_cb.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4813953",
      "metadata": {
        "scrolled": true,
        "id": "c4813953"
      },
      "outputs": [],
      "source": [
        "# 파일로 저장하기\n",
        "file = './data/CoffeeBean_매장정보.csv'\n",
        "df_cb.to_csv(file, encoding='cp949', mode='w', index=False)\n",
        "\n",
        "df = pd.read_csv(file, encoding='cp949')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02b76444",
      "metadata": {
        "id": "02b76444"
      },
      "source": [
        "--------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bdcd155",
      "metadata": {
        "id": "7bdcd155"
      },
      "source": [
        "끝"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}